---
title: "TEXT ANALYSIS"
output: html_notebook
---

This notebook is a replication of this e-book: https://www.tidytextmining.com/. There's little to no different analysis than what comes in this guide. Reason being that because text is a very specific data and the intro to basics will always be the same - any time we analise text data (twitter, blogs, magazines, facebook, surveys, interviews etc.) the starting point/basic analysis will always be the same. I chose to use tidytext because it's much more flexible than traditional bag-of-words approach.


# 1. First, as always, we need  to load the necessary libraries
```{r}
# install packages before loading
# I wuld ask the learners to run this before the session
#install.packages(c('tidytext','dplyr','textclean','ggplot2','reshape2','wordcloud', 'tm', 'textdata', 'tidyr', 'igraph', 'ggraph', 'widyr', 'topicmodels', dependencies=TRUE))
#install.packages("xml2") # to get xml2 to load to install the tm package

library(tidytext) 
library(tm)       
library(dplyr)     
library(textclean) 
library(ggplot2)   
library(reshape2)  
library(wordcloud) 
library(textdata)
library(tidyr)
library(igraph)
library(ggraph)
library(widyr)
library(topicmodels)
```


# 2. Now that we've loaded the libraries that we will need, we need to read the data into RStudio.
## To use the Twitter data on Apple, download  the .csv from decd.co/data-helpers "Data" folder, "Text analysis" folder contains the notebooks
```{r}
# Ensure that strings (text) are not read as factors in R, but as characters. This is required for any text analysis we do later.

# setwd("file path where your .csv file is located")
tweets <- read.csv("apple-tweets.csv", stringsAsFactors = FALSE) # For windows machines add:encoding = "UTF-8",  otehrwise the data imports with mistakes/weird characters


# Use the View function to view the dataframe in another table
View(tweets)


# What are our columns?
colnames(tweets)

```

# 3. Data Transformation

```{r}
# Text cleaning
# if we start with cleaning text column before extracting it from the dataset for the analysis it's easier to go back and add other columns of interest to the analysis

tweets$text <- tolower(tweets$text)
tweets$text <- removePunctuation(tweets$text, preserve_intra_word_contractions = FALSE, preserve_intra_word_dashes = TRUE, ucp = TRUE)
tweets$text <- replace_money(tweets$text, replacement = "")
tweets$text <- replace_url(tweets$text) # remove url
tweets$text <- replace_html(tweets$text) # removve html
tweets$text <- removeWords(tweets$text, words = c(stopwords("english"), "apple")) # removve stopwords
tweets$text <- replace_date(tweets$text, remove = TRUE)
tweets$text <- replace_number(tweets$text, remove = TRUE)
tweets$text <- gsub("[^\x01-\x7F]", "", tweets$text) # these three lines remove emojis, emoticons and words shorter than 2 letters
tweets$text <- gsub('\\p{So}|\\p{Cn}', '', tweets$text, perl = TRUE)
tweets$text <- gsub('\\b\\w{1,2}\\s','',tweets$text)# >2 letters
tweets$text <- stripWhitespace(tweets$text)
tweets$text <- removePunctuation(tweets$text, preserve_intra_word_contractions = FALSE, preserve_intra_word_dashes = TRUE, ucp = TRUE) # removePunctuation has to be run twice because some of the functions above will "release" punctuation 
# cleaning functions may be run out of sequence - going back to them if we se something odd in the text and adding others when need
```



```{r}
# How about we add more information to our text? Let's choose a column that's interesting to compare by
summary(as.factor(tweets$verified))
```



# Tidy text format
```{r}
#in order to turn it into a tidy text dataset, we first need to put tweet text into a data frame.
#language <- cldr::detectLanguage(tweets$text)

text_df <- tibble(line = as.numeric(rownames(tweets)), text=tweets$text, verified = as.factor(tweets$verified))

text_df
```

```{r}
# Within our tidy text framework, we need to both break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure. To do this, we use tidytext’s unnest_tokens() function.
text_tidy <- text_df %>%
  unnest_tokens(word, text)
text_tidy
```

```{r}
# Word frequencies
text_tidy %>%
  count(word, sort = TRUE)
```


```{r}
# Or we can put them in a graph
text_tidy %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


# Word cloud

```{r}
# Another way to represent most frequent words
text_tidy %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```



# Sentiment analysis
 Let's start with a simple positive-negative sentiment first
 
```{r}
# bing lexicon does just that!
get_sentiments("bing")
```
 

```{r}
# Sentiment and word counts
bing_word_counts <- text_tidy %>%
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```


```{r}
# We can also filter: just positive or just negative sentiment
bing_negative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative") # or just positive

text_tidy %>%
  inner_join(bing_negative) %>%
  count(word, sort = TRUE)
```

```{r}
# Visualising sentiment frequencies

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

```{r}
# Comparison cloud

text_tidy %>%
  #filter(verified==TRUE) %>%   # we can filter by if the account is veried or not to see if there's a difference in language used (a lot of unverified accounts are bots)
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

# More complex  - emotions
loughran - is a more complex lexicon that gives a wider range of dimentions 
NRC  -  5 emotional dimentions (similart ot what IBM Watson does) - code same as for bing, just replace
Afinn - assigns a sentiment score from -5 (very negative) to +5 (very positive)

NOTE: LEXICONS OTHER THAN BING ARE VERY FINNICKY - THEY MAY OR MAY NOT WORK ON DIFFERENT MACHINES - I AM LOOKING INTO IT, BUT DON'T HAVE A READY SOLUTION. MY ADVICE WOULD BE TO JUST USE BING AND MENTION THE OTHERS AND MOVE ON TO OTHER PARTS OF THE NOTEBOOK.

# Relationships between words: n-grams
Are there two words that come next to each other often? can we grasp some meaning from that?

```{r}
# to create bigrams we go back to our text_df and create bigrams from tweets rather than single words
text_bigrams <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  drop_na()

text_bigrams
```

```{r}
# most common bigrams
text_bigrams %>%
 count(bigram, sort = TRUE)
```

# Analysing bigrams

```{r}
# sometimes we may want to separate the bigrams to analyse them
bigrams_separated <- text_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_separated
```


```{r}
# what words appear next to a certain word - we can filter this information out 
bigrams_separated %>%
  filter(word1 == "bug") %>%  # or word1
  count(word2, sort = TRUE)
```

# Creating the bigram graphs
```{r}
# let's put the bigram counts into an object
bigram_counts <- bigrams_separated %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

```{r}
# Let's create a graph (you'll more about this in Network Analysis)
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph
```


```{r}
# We can very quickly to visualise it!
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```


```{r}
# or making it even prettier
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

# Word correlations
One useful function from widyr is the pairwise_count() function. The prefix pairwise_ means it will result in one row for each pair of words in the word variable. This lets us count common pairs of words co-appearing within the same section:
```{r}
# count words co-occuring
word_pairs <- text_tidy %>%
  pairwise_count(word, line, sort = TRUE)

word_pairs
```

The phi coefficient is equivalent to the Pearson correlation, which you may have heard of elsewhere, when it is applied to binary data

```{r}
# Let's look at the word correlations
word_cors <- text_tidy %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, line, sort = TRUE)

word_cors

# if there's time  - i would remove the "jewishladyblog enter win gift card giveaway" - include it in stopwords and rerun - clearly spam although this account is verified and looks like a real person...
```

```{r}
# what words correlate with "bug"
word_cors %>%
  filter(item1 == "bug" | item2 == "bug")
```

```{r}
# visualising correlations 
word_cors %>%
  filter(correlation > .4) %>% # we can choose the correlation level
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

# Topic modeling
Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language

LDA function may be a bit annoying - if it gets stuck, restart R and run the notebook again, sometimes if youu save the notebook throughout it doesn't run... 

```{r}
# for topic modeling we have to create a Document Term Matrix
text_dtm <- text_tidy %>%
  count(line, word, sort = TRUE) %>% 
  cast_dtm(line, word, n)
```


```{r}
# Let's create the topic model
text_lda <- LDA(text_dtm, k = 3, control = list(seed = 1234))
text_lda
```

```{r}
# we can extract the relevvant information about the topics
text_topics <- tidy(text_lda, matrix = "beta")
text_topics

# we still get a couple of rogue b-s even after data cleaning...
```


```{r}
#Let's have a look what these topics look like!

text_top_terms <- text_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

text_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```