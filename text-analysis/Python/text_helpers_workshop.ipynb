{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analytics - tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data and matrix manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For string manipulation\n",
    "import re \n",
    "import string\n",
    "\n",
    "# For text pre-processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Necessary dependencies from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For assigning sentiment polarity scores\n",
    "from textblob import TextBlob\n",
    "\n",
    "# For extracting features -- i.e. the document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some ML models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# For evaluation of ML models\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking above, we can see there's much to be done -- some questions we could ask ourselves:\n",
    " - What do we do with handles? I.e. @Apple \n",
    " - What do we do with punctuation? I.e. !?.-# \n",
    " - How do we handle words spelt incorrectly? I.e. that vs. thattt\n",
    " - What do we do with Emojis? I.e. :), :-) etc.\n",
    " - What about words that have inflectional changes? Do we keep them or return them to their base? See [NLP Stanford on Stemming and Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be a good idea to begin with a sample sententce, and see how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'This is the pre-processing stages          @ApplE. #whataday This is SOOOO EXCI~TING. Cows. Thats all i have to say come find me at https://decoded.co'\n",
    "\n",
    "# word_tokenize(sample)\n",
    "# sent_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('\\s+', ' ', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the lemmatizer function - by default it lemmatizes nouns, for e.g.:\n",
    "print(WordNetLemmatizer().lemmatize('cars'))\n",
    "\n",
    "# But it can also be adjusted to lemmatize adjectives by setting the 'pos' parameter to 'a':\n",
    "print(WordNetLemmatizer().lemmatize('cleanse', pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PorterStemmer().stem('cleanse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning/Transformation function\n",
    "\n",
    "Now let's create a function that does all the cleaning/transformation in one go.\n",
    "\n",
    "Remember we can always go back and change our choices later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sample_text):\n",
    "\n",
    "    # First, let's try changing everything to lower case\n",
    "    sample_text = sample_text.lower()    \n",
    "    \n",
    "    # Let's now replace a select few symbols with white space to make things easier for ourselves\n",
    "    sample_text = re.sub('[#@~?!]',' ',sample_text)\n",
    "    \n",
    "    # We then strip any extra white space\n",
    "    sample_text = re.sub('\\s+',' ', sample_text)\n",
    "    \n",
    "    # Remove all URLs: \n",
    "    sample_text = sample_text.replace('https?:[A-Za-z0-9/.]*', '')\n",
    "    \n",
    "    # Then lemmatize our words -- note,  stemming was deemed too crude here, and therefore not chosen\n",
    "    sample_text = WordNetLemmatizer().lemmatize(sample_text, pos = 'n')\n",
    "    \n",
    "    # Try lemmatizing adjectives & verbs\n",
    "    \n",
    "    # Now that we transformed our text, we need to tokenize it. Let's treat each word as a token.\n",
    "    words = word_tokenize(sample_text)\n",
    "    \n",
    "    # As we now have a list of words,  we can go ahead and find and remove those words that also belong to the \n",
    "    # stopwords list from the NLTK corpus\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    \n",
    "    # We then proceed to joining those list of words, back to 'free text'  or string format\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned our corpus of tweets, we have put it in a form that can be used by our machine learning models. The model we'll be using is called the **bag of words** model.\n",
    "\n",
    "This involves creating a document-term matrix -- a matrix with **documents** or tweets as the rows, and unique **terms** as the columns -- which we use the _CountVectorizer()_ function for from the NLTK package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify that we need no more than 10000 features -- i.e. 1000 unique terms. Of course, this is an arbitrary number\n",
    "# feel free to play around with this parameter!\n",
    "\n",
    "# We also specify the min_df parameter to be 0.01. This means that our terms should at least be used in 1% of our \n",
    "# tweets.\n",
    "\n",
    "# Finally, we specify an ngram_range of 1. This means that we're only looking for words -- an ngram_range of (1,2) \n",
    "# would include both words (length = 1) and phrases or combinations of words of length = 2 \n",
    "vector = CountVectorizer(max_features= 10000 , min_df=0.01, ngram_range= (1,1))\n",
    "\n",
    "# We use the fit_transform() function to apply the above to our tweets\n",
    "bag_of_words = vector.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Understand – Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's lots to explore here! Let's begin with a few visualisations that could be of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sum of occurences of each term\n",
    "sum_of_words = bag_of_words.sum(axis= 0)\n",
    "\n",
    "# Create a list of tuples where each element represents the term in question and how many times it occurs in our \n",
    "# corpus.\n",
    "words_freq = [(word, sum_of_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
    "\n",
    "# Sort in decreasing order of frequency.\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignoring top word (which is \"apple\" in this case)\n",
    "top_words = words_freq[1:30]\n",
    "\n",
    "word = []\n",
    "count = []\n",
    "\n",
    "for i, j in top_words: \n",
    "    word.append(i)\n",
    "    count.append(j)\n",
    "\n",
    "# Asjusting figure size\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "# Plotting a barplot of most frequent words using Seaborn\n",
    "sns.barplot(x = count, y = word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to plot most frequent words is through use of Wordclouds\n",
    "words_dict = {}\n",
    "for k,v in top_words:\n",
    "    words_dict[k] = int(v)\n",
    "\n",
    "# Using the WordCloud library\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color=\"white\").generate_from_frequencies(words_dict)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Understand – Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we visualised most frequent words, we can delve into more interesting aspects, like sentiment. \n",
    "\n",
    "\n",
    "To do this, we'll be using a library called **Textblob**. The package assigns a _sentiment polarity score_ from -1 to 1 to each of our words. It then calculates the sum total sentiment polarity for each tweet by averaging the scores for the terms in the tweet in question. \n",
    "\n",
    "Textblob assigns sentiment scores based on pre-populated lexicon, or dictionary of words that have been previously assigned scores by humans -- no magic here! For more information on how the scoring is calculated read more [here](https://planspace.org/20150607-textblob_sentiment/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We create a list of sentiment scores on each of our tweets using Textblob\n",
    "sentiments = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    analysis = TextBlob(tweet)\n",
    "    sentiments.append(analysis.sentiment.polarity)\n",
    "\n",
    "# We add that list to a new dataframe of our tweets\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "\n",
    "tweets_df['sentiments'] = sentiments\n",
    "\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that categorises sentiment -- 'Negative' if <0, 'Positive' if >0 and 'Neutral' if equal to 0.\n",
    "categories = []\n",
    "\n",
    "for sentiment in tweets_df['sentiments']: \n",
    "    if sentiment > 0:\n",
    "        categories.append('Positive')\n",
    "    elif sentiment < 0: \n",
    "        categories.append('Negative')\n",
    "    else:\n",
    "        categories.append('Neutral')\n",
    "        \n",
    "\n",
    "tweets_df['sentiment_category'] = categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Sentiment – Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've explored our data (although not exhaustively) let's see if we can build a machine learning model that is able to predict the sentiment of a tweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split into a training and a testing set as before -- specifying: \n",
    " - test_size = 0.3 -- i.e. we train on 70% of our dataset and test on 30%\n",
    " - a random_state = 123 -- for our results to be reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the train_test_split function\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweets_df['text'], tweets_df['sentiment_category'],  \n",
    "                                                   test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply our bag of words model to our entire text using the _fit_ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit bag of words model (Countvectorizer) to full text first\n",
    "vector.fit(tweets_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply same feature transformation to both x_train, and x_test\n",
    "x_train_bow = vector.transform(x_train)\n",
    "\n",
    "x_test_bow = vector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the same number of features \n",
    "print(x_train_bow.shape, x_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call on sklearn's machine learning algorithms!\n",
    "\n",
    "One basic model that is typically used for text modelling is _Naive Bayes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Communicate\n",
    "\n",
    "We've now modelled our tweets using the bag-of-words model. There are many more ways we can explore this further. To name a few directions: \n",
    "\n",
    " - We can improve the accuracy of our current models\n",
    " - We can try other machine learning models\n",
    " - We can try different pre-processing techniques\n",
    " - There are other modelling techniques besides the bag of words model -- maybe try TFIDF or Word2Vec? Lots you can explore here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
