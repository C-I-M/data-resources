{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analytics - tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Jupyter Notebooks\n",
    "\n",
    "DO: Try printing \"hello world\"! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data and matrix manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For string manipulation\n",
    "import re \n",
    "import string\n",
    "\n",
    "# For text pre-processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Necessary dependencies from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For assigning sentiment polarity scores\n",
    "from textblob import TextBlob\n",
    "\n",
    "# For extracting features -- i.e. the document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source and Manage Data\n",
    "\n",
    "DO: load in the CSV data using pandas, print out the first 6 rows using the head() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking above, we can see there's much to be done -- some questions we could ask ourselves:\n",
    " - What do we do with handles? I.e. @Apple \n",
    " - What do we do with punctuation? I.e. !?.-# \n",
    " - How do we handle words spelt incorrectly? I.e. that vs. thattt\n",
    " - What do we do with Emojis? I.e. :), :-) etc.\n",
    " - What about words that have inflectional changes? Do we keep them or return them to their base? See [NLP Stanford on Stemming and Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be a good idea to begin with a sample sentence, and see how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'This is the pre-processing stages    @ApplE. #whataday This is SOOOO EXCI~TING. Cows. Thats all i have to say. Come find me at https://decoded.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_tokenize - transforms our string/text into a list of words (separated by a white space), where each word is \n",
    "# called a token.\n",
    "word_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenize - transforms our string/text into a list of sentence (separated by a '.'), where each sentence is \n",
    "# a token.\n",
    "sent_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms each character to its lower case form\n",
    "sample.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to capture patterns in our text data, we use a handy tool called _regex_ or regular expressions. This is a whole interesting area of text analytics worthy of exploration. \n",
    "\n",
    "For reference on some regular expressions and how to use them: [W3 Schools Regex](https://www.w3schools.com/python/python_regex.asp)\n",
    "\n",
    "For trialling various regex patterns and combinations on more text:  [Regexr](https://www.regexr.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes URLs from the sample text:\n",
    "re.sub('http\\S+', '', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitutes a specified characted with another specified character of your choice\n",
    "re.sub('@', '@@', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes extra white space\n",
    "re.sub('\\s+', ' ', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing a select few symbols\n",
    "re.sub('#|!|~','', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we may also want to consider exploring 'stopwords'. Those are words like 'and', 'you', and 'I' that add little value to our model and would only clutter our corpus. \n",
    "\n",
    "The NLTK Library has a very handy pre-populated dictionary of such words that we can use to our advantage.\n",
    "\n",
    "They also have a [list of other corpora](http://www.nltk.org/nltk_data/) that may be useful for future text analytics projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've played around with text preprocessing and have some idea of what we can do, let's go ahead and put it all in a function.  \n",
    "\n",
    "Remember we can always go back and change our choices later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sample_text):\n",
    "    \n",
    "    # Let's remove all of the URLs from the text: \n",
    "    sample_text = re.sub('http\\S+', '', sample_text)\n",
    "    \n",
    "    # Given a sample text (as a string), we first substitute a select few sybmols with white space\n",
    "    sample_text = re.sub(r'[#|@|-|?|!]',r' ',sample_text)\n",
    "    \n",
    "    # We then strip extra white space\n",
    "    sample_text = re.sub('\\s+',' ', sample_text)\n",
    "    \n",
    "    # Then change everything to lower case\n",
    "    sample_text = sample_text.lower()\n",
    "    \n",
    "    # Now that we transformed our text, we need to tokenize it. Let's treat each word as a token.\n",
    "    words = word_tokenize(sample_text)\n",
    "    \n",
    "    # As we now have a list of words,  we can go ahead and find and remove those words that also belong to the \n",
    "    # stopwords list from the NLTK corpus\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    \n",
    "    # We then proceed to joining those list of words, back to 'free text'  or string format\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO: Let's apply the clean_text function to our tweets! First let's copy the original text so we don't lose it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned our corpus of tweets, we have put it in a form that can be used by our machine learning models. The model we'll be using is called the **bag of words** model.\n",
    "\n",
    "This involves creating a document-term matrix -- a matrix with **documents** or tweets as the rows, and unique **terms** as the columns -- which we use the _CountVectorizer()_ function for from the NLTK package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify that we need no more than 10000 features -- i.e. 1000 unique terms. Of course, this is an arbitrary number\n",
    "# feel free to play around with this parameter!\n",
    "\n",
    "# We also specify the min_df parameter to be 0.01. This means that our terms should at least be used in 1% of our \n",
    "# tweets.\n",
    "\n",
    "# Finally, we specify an ngram_range of 1. This means that we're only looking for words -- an ngram_range of (1,2) \n",
    "# would include both words (length = 1) and phrases or combinations of words of length = 2 \n",
    "vector = CountVectorizer(max_features= 10000 , min_df=0.01, ngram_range= (1,1))\n",
    "\n",
    "# We use the fit_transform() function to apply the above to our tweets\n",
    "bag_of_words = vector.fit_transform(tweets)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a sparse matrix of:\n",
    " - 9991 documents - i.e tweets\n",
    " - 241 unique terms\n",
    " \n",
    "Where each cell represents the number of times the term in question occurs in the document in question. It's sparse as there are inevitably, lots of zeros!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO: Use the `get_feature_names()` function to view the features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Understand – Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's lots to explore here! Let's begin with a few visualisations that could be of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sum of occurences of each term\n",
    "sum_of_words = bag_of_words.sum(axis= 0)\n",
    "\n",
    "# Create a list of tuples where each element represents the term in question and how many times it occurs in our \n",
    "# corpus.\n",
    "words_freq = [(word, sum_of_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
    "\n",
    "# Sort in decreasing order of frequency.\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignoring top word (which is \"apple\" in this case)\n",
    "top_words = words_freq[1:30]\n",
    "\n",
    "word = []\n",
    "count = []\n",
    "\n",
    "for i, j in top_words: \n",
    "    word.append(i)\n",
    "    count.append(j)\n",
    "\n",
    "# Asjusting figure size\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "# Plotting a barplot of most frequent words using Seaborn\n",
    "sns.barplot(x = count, y = word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to plot most frequent words is through use of Wordclouds\n",
    "words_dict = {}\n",
    "for k,v in top_words:\n",
    "    words_dict[k] = int(v)\n",
    "\n",
    "# Using the WordCloud library\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color=\"white\").generate_from_frequencies(words_dict)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try again!\n",
    "words_dict = {}\n",
    "for k,v in top_words:\n",
    "    words_dict[k] = int(v)\n",
    "\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color=\"white\").generate_from_frequencies(words_dict)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Understand – Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we visualised most frequent words, we can delve into more interesting aspects, like sentiment. \n",
    "\n",
    "\n",
    "To do this, we'll be using a library called **Textblob**. The package assigns a _sentiment polarity score_ from -1 to 1 to each of our words. It then calculates the sum total sentiment polarity for each tweet by averaging the scores for the terms in the tweet in question. \n",
    "\n",
    "Textblob assigns sentiment scores based on pre-populated lexicon, or dictionary of words that have been previously assigned scores by humans -- no magic here! For more information on how the scoring is calculated read more [here](https://planspace.org/20150607-textblob_sentiment/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "DO: Create a list of sentiment scores on each of our tweets using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO: Find the most positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO: Use `sns.distplot` to plot the distribution of sentiments of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO: Create a new column that categorises sentiment -- 'Negative' if <0, 'Positive' if >0 and 'Neutral' if equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO: Use `sns.countplot` to create a countplot of each different sentiment category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
